{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Model for Document-Based Question Answering\n",
        "**Author:** Khai Ta  \n",
        "**Date:** November 2024\n",
        "\n",
        "In this project, I implement a Retrieval-Augmented Generation (RAG) model using OpenAI's API to answer questions about Lionel Messi. The model retrieves relevant text chunks from a document, generates embeddings for both the text and the question, calculates cosine similarity to find relevant chunks, and sends a contextualized prompt to the LLM for a final answer."
      ],
      "metadata": {
        "id": "IzmnI5JXANto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importing Libraries\n",
        "\n",
        "We import the `openai` library for using OpenAI's API and `numpy` for handling numerical data in embeddings."
      ],
      "metadata": {
        "id": "oCRGEerbA9Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DZ9NkBlHB22I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setting Up OpenAI API Key\n",
        "\n",
        "We set up the OpenAI API key to access embeddings and language model completions. Make sure to replace `\"YOUR_API_KEY\"` with your actual OpenAI API key."
      ],
      "metadata": {
        "id": "6q6YgXpuB0E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=\"YOUR_API_KEY\")"
      ],
      "metadata": {
        "id": "4zc7HonMDdwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loading and Chunking the Document\n",
        "\n",
        "We load the document `messi.txt` and divide it into smaller chunks of text. This step ensures that the document is manageable for embedding generation and makes it easier to retrieve relevant sections based on questions."
      ],
      "metadata": {
        "id": "HxGDjIzAEMX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_chunk(file_path, chunk_size=100):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "    result = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "      result.append(text[i:i + chunk_size])\n",
        "\n",
        "chunks = load_and_chunk(\"messi.txt\")"
      ],
      "metadata": {
        "id": "mTdV2YilETK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generating Embeddings for Document Chunks\n",
        "\n",
        "Using OpenAI’s Embedding API, we create embeddings for each chunk in `messi.txt`. These embeddings represent each chunk as numerical vectors, which we later use to find the most relevant chunks based on similarity to the question."
      ],
      "metadata": {
        "id": "_rWhVw2UEY9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text):\n",
        "    response = client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
        "    return response.data[0].embedding\n",
        "\n",
        "embeddings_dict = {}\n",
        "for chunk in chunks:\n",
        "    embeddings_dict[chunk] = get_embedding(chunk)"
      ],
      "metadata": {
        "id": "Xa88smZxEikl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Calculating Cosine Similarity and Retrieving Relevant Chunks\n",
        "\n",
        "We generate an embedding for the question and then calculate cosine similarity between the question's embedding and each chunk's embedding. This allows us to identify the top 5 most relevant chunks, which we will include in the prompt for the LLM."
      ],
      "metadata": {
        "id": "bBAw3P41H0Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def find_most_relevant_chunks(question, embeddings_dict, top_n=5):\n",
        "    question_embedding = get_embedding(question)\n",
        "    similarities = []\n",
        "\n",
        "    for chunk, emb in embeddings_dict.items():\n",
        "        similarity = cosine_similarity(question_embedding, emb)\n",
        "        similarities.append((chunk, similarity))\n",
        "\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_chunks = []\n",
        "    for chunk, _ in similarities[:top_n]:\n",
        "        top_chunks.append(chunk)\n",
        "\n",
        "    return top_chunks\n",
        "\n",
        "question = \"What achievements has Messi had with the Argentina national team?\"\n",
        "top_chunks = find_most_relevant_chunks(question, embeddings_dict)"
      ],
      "metadata": {
        "id": "98FuzE3CH7l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Creating the Prompt for the Language Model\n",
        "\n",
        "We combine the question with the top relevant chunks to form a single prompt. This prompt will provide the LLM with both the question and the context necessary to generate an accurate answer."
      ],
      "metadata": {
        "id": "y664ugr6ILDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(question, top_chunks):\n",
        "    prompt = f\"Question: {question}\\n\\nContext:\\n\" + \"\\n\".join(top_chunks)\n",
        "    return prompt\n",
        "\n",
        "prompt = create_prompt(question, top_chunks)"
      ],
      "metadata": {
        "id": "HHRGPW7zIPTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Querying the Language Model for an Answer\n",
        "\n",
        "Using the prompt, we query OpenAI’s language model (e.g., GPT-4) to retrieve an answer based on the question and contextual chunks from the document."
      ],
      "metadata": {
        "id": "Y9xETghTIUCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_from_llm(prompt):\n",
        "    response = client.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "answer = get_answer_from_llm(prompt)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "9Usw5IkUIWyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "We successfully used OpenAI's API to extract relevant information from a document and generate accurate answers to specific questions. By chunking the document, generating embeddings, and calculating cosine similarity, we identified the most relevant sections of the text. This process allowed us to create a prompt with contextual information, which was then used to query the language model for an informed response."
      ],
      "metadata": {
        "id": "TPY_0XsmOCUN"
      }
    }
  ]
}