{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeub4nSHjoq9"
   },
   "source": [
    "# ðŸ” Objective:\n",
    "\n",
    "This project will introduce you to Retrieval Augmented Generation and how it can be used to expand the knowledge base of an existing pretrained LLM.\n",
    "\n",
    "# ðŸ§ Glossary:\n",
    "\n",
    "We're going to be using some rather fancy sounding words that you may not have come across before. I'd highly reccommend googling them, but feel free to refer to this cheat sheet if you forget.\n",
    "\n",
    "\n",
    "1.   Vector: Think of this as a list of numbers specifically used to represent co-ordinates. So a vector containing the co-ordinates to a point at (2,8) would be [2, 8]. Now we'll often be using arrays of vectors, we'll also be using a library called numpy, which supports 2D arrays (basically an array of arrays) much better than regular Python.\n",
    "\n",
    "2.   Embeddings:\n",
    "\n",
    "*   A huge part of RAG is semantic search (i.e. searching by meaning). This is the core of what makes RAG so powerful, as it can tell us how similar two sentences are based on what they mean, even if they are worded differently.\n",
    "\n",
    "*   The way this is done is by using an embedding model to convert text (or images) to points in space. The closer two points are, the closer in meaning their corresponding texts are.\n",
    "\n",
    "*   We will be storing the co-ordinates of the points in vectors, hence the need for a vector database.\n",
    "\n",
    "*   Note: While it is helpful to think of the points generated by an embedding model as points in 3 dimensions, most embedding models generate points in higher dimensions; the model we are using generates points in 384 dimensions! So instead of our points having an x, y and z co-ordinate, they will have x, y, z, w, v,..... co-odinates.\n",
    "\n",
    "3. LLM: Large Language Models (LLM) can generate text based on a provided prompt. Sound familiar? It should; ChatGPT is a Large Language Model! We'll be using an LLM to actually answer a users question. The problem here is that LLMs aren't all knowing; they can only answer questions based on what they've been trained on. A way to remedy this is to use techniques like RAG to work out what snippets of text from an external source are most similar to the users question and feeding them to the LLM along with the users question. The LLM can use this data to generate a natural sounding answer.\n",
    "\n",
    "4. Chunks:\n",
    "* A chunk is simply a smaller piece of a larger piece of text. The reason you'd want to break down, say, a book or a pdf is to find relevant pieces of information in it. After all, we don't want to throw an entire PDF at our LLM and have it decipher all of it, now do we?\n",
    "\n",
    "* We'll use a bunch of functions that we'll get to later to work out which chunks are the most relevant to our question and pass just those to our LLM.\n",
    "\n",
    "* The size of a chunk is entirely up to the programmer, but you want a chunk to be big enough for it to actually have some sort of meaning in it on its own, while not being big enought to contain too many different kinds of information in it.\n",
    "\n",
    "\n",
    "# ðŸ“Œ What You'll Do:\n",
    "\n",
    "\n",
    "\n",
    "1.   Create a rudimentary vector database by defining the functions in the template class below\n",
    "2.   Convert a pdf of a stock trading tutorial to a bunch of small strings (called chunks. we'll get to why later).\n",
    "3.   Use this vector database to store the embeddings of the chunks.\n",
    "4. Handle user queries by querying our database for the most relevant chunks and feeding them to our LLM to generate an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0aVeoPaRrttW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 302 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /Users/khaita/Library/Python/3.9/lib/python/site-packages (from pypdf) (4.13.0)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.6.0-cp39-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.5 MB 62.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Downloading pillow-11.1.0-cp39-cp39-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1 MB 32.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.5.0 in /Users/khaita/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (4.13.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.2 MB 37.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.1 MB 38.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.3 MB 109.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481 kB 38.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194 kB 40.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/khaita/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 32.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.2 MB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 536 kB 38.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.3 MB 40.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.7 MB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 418 kB 30.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284 kB 37.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128 kB 25.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 166 kB 31.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70 kB 27.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl (197 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197 kB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 301 kB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, tqdm, requests, pyyaml, fsspec, filelock, numpy, mpmath, MarkupSafe, huggingface-hub, tokenizers, threadpoolctl, sympy, scipy, safetensors, regex, networkx, joblib, jinja2, transformers, torch, scikit-learn, Pillow, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.1 idna-3.10 jinja2-3.1.6 joblib-1.4.2 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.13.1 sentence-transformers-4.0.1 sympy-1.13.1 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.50.3 urllib3-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#library installations. we'll be using sentence_transformers for our embedding model and pypdf to read a pdf of our choosing.\n",
    "\n",
    "%pip install pypdf\n",
    "%pip install sentence_transformers\n",
    "\n",
    "#fun fact: the ! is used to signify that these are shell commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cmtAzj17opiL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khaita/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/khaita/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader as PDFReader\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "#I highly recommend sticking to this LLM; its small and runs decent in a colab notebook, especially if you have GPU acceleration enabled.\n",
    "LLM_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#Same goes for our embedding model.\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L12-v2\"\n",
    "HF_API_KEY = \"YOUR-HF-API-KEY-HERE\"#Not a necessity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fP3kPw_7Aq-T"
   },
   "outputs": [],
   "source": [
    "login(token=HF_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9bCcR4eMyg1Y"
   },
   "outputs": [],
   "source": [
    "class VectorDB:\n",
    "  embedModel = None#we will be using embedModel to refer to our embedding model through out this class. Its value is initialized in the constructor\n",
    "\n",
    "  nDim = 0#number of dimensions. Remember how I told you the embedding model generates points in space?\n",
    "          #nDim will contain the number of dimensions of the space these points are generated in. It will come in handy later\n",
    "\n",
    "  _embeddings = None#This is the numpy array that will actually store our embeddings. the underscore at the start of the variable name\n",
    "                    #signifies that this a private variable; i.e it should not be directly accessed outside of this class.\n",
    "\n",
    "  _strings : list[str] = []#we will also be storing the strings that we are generating embeddings for.\n",
    "\n",
    "\n",
    "  #note: _embeddings is a numpy array. THIS IS NOT THE SAME AS A REGULAR PYTHON ARRAY.\n",
    "  #      _strings on the other hand is a regular ole Python array.\n",
    "\n",
    "\n",
    "\n",
    "  def __init__(self, model_name : str) -> None:#DO NOT CHANGE. This is the constructor funtion.\n",
    "    #we've set up a lot of the boilerplate code for you; this code instantiates all the class variables and loads the embedding model.\n",
    "    self.embedModel = SentenceTransformer(model_name)\n",
    "    self.nDim = self.embedModel.get_sentence_embedding_dimension()\n",
    "\n",
    "    self._embeddings = numpy.ndarray((0, self.nDim), dtype = numpy.float32) #sets _embeddings to an array of an array.\n",
    "    #The first number (0) represents the number of arrays that are stored in _embeddings. 0 for now as its empty.\n",
    "    #nDim refers to the number of values in each of those subarrays. We need one to represent each dimension, the same way you'd need an array with 3 values to represent a 3d point.\n",
    "\n",
    "    self._strings = []\n",
    "\n",
    "  def addToDatabase(self, input : list[str]):\n",
    "    '''\n",
    "    Your code should do two things here:\n",
    "    1. Convert the strings in the input array to embeddings and add them to the _embeddings array\n",
    "    2. Store their corresponding strings (IN THE SAME ORDER) in the _strings array\n",
    "\n",
    "    Hint: the documentation for the SentenceTransformers library can be found here: https://sbert.net/docs/package_reference/sentence_transformer/index.html\n",
    "    If you want to know which function to call for anything related to the embedding model (eg: generating embeddings) have a look at this.\n",
    "    '''\n",
    "    #Your code goes here!\n",
    "\n",
    "    #The pass command will cause everything within this function thats after pass to be ignored.\n",
    "    #Delete it once you start coding or keep all your code above it\n",
    "    # Generate embeddings for the input strings\n",
    "    new_embeddings = self.embedModel.encode(input)\n",
    "\n",
    "    # Add the new embeddings and strings to the existing arrays\n",
    "    if self._embeddings.size == 0:\n",
    "      self._embeddings = new_embeddings\n",
    "      self._strings = input\n",
    "    else:\n",
    "      self._embeddings = numpy.concatenate((self._embeddings, new_embeddings), axis=0)\n",
    "      self._strings.extend(input)\n",
    "\n",
    "  def clearDatabase(self):\n",
    "    '''\n",
    "    This function should clear the database by emptying the _embeddings and _string arrays.\n",
    "    '''\n",
    "    #Your code goes here!\n",
    "\n",
    "    # Reset embeddings and strings arrays\n",
    "    self._embeddings = numpy.ndarray((0, self.nDim), dtype=numpy.float32)\n",
    "    self._strings = []\n",
    "\n",
    "  def euclideanSim(self, x, y, dimensions: int):\n",
    "    '''\n",
    "    This function calculates how close two points are using euclidean distance.\n",
    "\n",
    "    Euclidean distance isn't anything fancy; it's the most basic method for comparing the distance between two points.\n",
    "    You may have seen it being used like this: âˆš((x2 - x1)Â²  +  (y2 - y1)Â²) for measuring distances in 2-D.\n",
    "\n",
    "    Your function should do the same thing, but in nDim dimensions instead.\n",
    "\n",
    "    Keep in mind that this function is meant to return similarity i.e the opposite of distance. (This isn't mandatory; just rename the function to avoid confusion if you'd rather just have it return distances)\n",
    "    After all, if two points are close to each other, their respective texts must be similar in meaning.\n",
    "    '''\n",
    "    #Your code goes here!\n",
    "\n",
    "    # Calculate Euclidean distance between two points\n",
    "    distance = numpy.sqrt(numpy.sum((x - y) ** 2))\n",
    "\n",
    "    # Return similarity (opposite of distance)\n",
    "    return 1 / (1 + distance)\n",
    "\n",
    "  def search(self, input : str, n_return = 1):\n",
    "    '''\n",
    "    This is the biggest function here by far.\n",
    "    This functions job is to find the n closest to the input in our database\n",
    "\n",
    "    This will be done by generating an embedding for our input, and finding the n closest points to it and what pieces of text are associated to them.\n",
    "\n",
    "    It should return a tuple (tRText, tRSim).\n",
    "    tRText will contain the n closest pieces of text and tRSim will contain their respective similarities to the query\n",
    "    '''\n",
    "    tRText = []\n",
    "    tRSim = numpy.array([])\n",
    "\n",
    "    #Your code goes here!\n",
    "\n",
    "    tRText = []\n",
    "    tRSim = numpy.array([])\n",
    "    \n",
    "    # Generate embedding for the input\n",
    "    query_embedding = self.embedModel.encode(input)\n",
    "\n",
    "    # Calculate similarities with all stored embeddings\n",
    "    similarities = []\n",
    "    for i in range(len(self._strings)):\n",
    "        sim = self.euclideanSim(query_embedding, self._embeddings[i], self.nDim)\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    # Get indices of n highest similarities\n",
    "    top_indices = numpy.argsort(similarities)[-n_return:][::-1]\n",
    "    \n",
    "    # Get the strings and similarities for the top matches\n",
    "    tRText = [self._strings[i] for i in top_indices]\n",
    "    tRSim = numpy.array(similarities)[top_indices]\n",
    "\n",
    "    return(tRText, tRSim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "27vH301F-7rd"
   },
   "outputs": [],
   "source": [
    "vDB = VectorDB(model_name = EMBEDDING_MODEL)\n",
    "#this code calls the constructor and sets vDB to be an instance of our vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rBisrmv2rhWt"
   },
   "outputs": [],
   "source": [
    "def chunksFromText(text : str):\n",
    "  '''\n",
    "  Your goal for this function is to convert the text of a page to individual chunks and store these chunks in our vector database.\n",
    "\n",
    "  Q: What do CHUNK_SIZE and CHUNK_OVERLAP mean?\n",
    "\n",
    "  A: CHUNK_SIZE is just the number of characters we are storing in each chunk. CHUNK_OVERLAP refers to the number of characters\n",
    "  shared between sequential chunks. I.E for a chunk overlap of 50 the first chunk will store store characters 0 to 500, the second 450 to 950,\n",
    "  the third 900 to 1400 and so on. Feel free to play around with these values; I chose them at random.\n",
    "\n",
    "  Also note that chunking by character count isn't the only approach that you can use. Another method is to have each chunk be a sentence,\n",
    "  multiple sentences, or an entire paragraph. Again, feel free to experiment!\n",
    "  '''\n",
    "\n",
    "  CHUNK_SIZE = 500\n",
    "  CHUNK_OVERLAP = 50\n",
    "\n",
    "  docChunks = []\n",
    "\n",
    "  #Your code goes here!\n",
    "  \n",
    "  CHUNK_SIZE = 500\n",
    "  CHUNK_OVERLAP = 50\n",
    "  \n",
    "  docChunks = []\n",
    "  start = 0\n",
    "  \n",
    "  while start < len(text):\n",
    "      end = min(start + CHUNK_SIZE, len(text))\n",
    "      chunk = text[start:end].strip()\n",
    "      if chunk:\n",
    "          docChunks.append(chunk)\n",
    "      start = end - CHUNK_OVERLAP\n",
    "  \n",
    "  vDB.addToDatabase(input = docChunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D1HNuSrLf0TJ"
   },
   "outputs": [],
   "source": [
    "def chunksFromPDF(path = \"\", startPage = 0, endPage = None):\n",
    "  '''\n",
    "  Here's the documentation for the PDFReader library: https://pdfreader.readthedocs.io/en/latest/tutorial.html\n",
    "  Your goal for this function is to extract text from each page in the PDF and call chunksFromText on the text to convert it to chunks and store\n",
    "  it in the vector database.\n",
    "\n",
    "  Quick reference: pdf = PDFReader(path) #gets us a a PDFReader object that we can call functions on\n",
    "                   pdf.pages #big array containing all the pages\n",
    "                   page = pdf.pages[0] #gets first page\n",
    "                   text = page.extract_text() #gets text from the page\n",
    "\n",
    "  You are required to clean the text from each page, as well as decide which pages are actually relevant.\n",
    "  Examples of things you should get rid of (Not exhaustive):\n",
    "  1. Headers, footers,\n",
    "  2. Indexes,\n",
    "  3. Title page,\n",
    "  etc.\n",
    "  '''\n",
    "  #Your code goes here!\n",
    "\n",
    "  pdf = PDFReader(\"/Users/khaita/Documents/PSU/NAiSS MLB/Spring 2025/project-3-rag-chatbot-khai-ta/Understanding Stocks.pdf\")\n",
    "    \n",
    "  if endPage is None:\n",
    "      endPage = len(pdf.pages)\n",
    "  \n",
    "  for i in range(startPage, endPage):\n",
    "      page = pdf.pages[i]\n",
    "      text = page.extract_text()\n",
    "      \n",
    "      # Basic text cleaning\n",
    "      text = text.strip()\n",
    "      # Remove common headers/footers\n",
    "      text = text.replace(\"Page\", \"\").replace(\"Â©\", \"\").replace(\"All rights reserved\", \"\")\n",
    "      # Remove multiple spaces\n",
    "      text = ' '.join(text.split())\n",
    "      \n",
    "      if text:  # Only process non-empty pages\n",
    "          chunksFromText(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c3A96az1-BvT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "answerBot = pipeline(\"text-generation\", model = LLM_MODEL, trust_remote_code=True) # This bit of code loads our LLM into memory; we can access it using\n",
    "                                                                                   # the answerBot object.\n",
    "                                                                                   #The model we are using is 2.2GB, so make sure you've got a decent WiFi connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qbZ1bXEtDe8J"
   },
   "outputs": [],
   "source": [
    "def generateAnswer(question):\n",
    "  '''\n",
    "  Documentation for the pipeline object (answerBot) we created two lines ago: https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/pipelines#transformers.TextGenerationPipeline\n",
    "\n",
    "  This is it, the final piece of the puzzle. This function will take a question, find out which chunks from our vector database\n",
    "  are most relevant to it, and ask our LLM the original question with the context of the text from the relevant chunks.\n",
    "\n",
    "  Have it return a string with the LLMs answer.\n",
    "  '''\n",
    "  #Your code goes here!\n",
    "\n",
    "  # Get relevant chunks from the database\n",
    "  relevant_chunks, _ = vDB.search(question, n_return=5)\n",
    "    \n",
    "  # Create context from relevant chunks\n",
    "  context = \"\\n\".join(relevant_chunks)\n",
    "    \n",
    "  # Create prompt with context and question\n",
    "  prompt = f\"\"\"Based on the following context, please answer the question. If the answer cannot be found in the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "# Generate answer using the LLM\n",
    "  output = answerBot(\n",
    "      prompt, \n",
    "      max_length=500, \n",
    "      do_sample=True, \n",
    "      temperature=0.7,\n",
    "      truncation=True,\n",
    "      num_return_sequences=1\n",
    "  )[0]['generated_text']\n",
    "  \n",
    "  # Extract just the answer part and clean it\n",
    "  answer = output.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "  # Remove any additional questions or prompts\n",
    "  answer = answer.split(\"Question:\")[0].strip()\n",
    "\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9wJPbBVa3pb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 401k is a retirement savings plan offered by many companies, typically through their paycheck deduction. It allows employees to set aside a portion of their paycheck, typically every paycheck, and then receive matching funds from the company. The funds can be used to invest in stocks, bonds, or other investment vehicles. The employee also receives a tax break for the contributions made to the 401k.\n"
     ]
    }
   ],
   "source": [
    "print(generateAnswer(\"What is a 401k?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc4plZnA-2nB"
   },
   "source": [
    "ðŸ“Œ Wrapping Up\n",
    "\n",
    "And that should be it! Try calling the generate answer function we just defined with a finance related question.\n",
    "\n",
    "I'd recommend asking it what a 401(k) is. The PDF we used uses a fancy bit of unicode to write 59.5 (which is how old you have to be before you start paying taxes on a 401k), and if our code is working properly, the LLM will work that character into its answer.\n",
    "\n",
    "I hope you had fun working on this project and learned something new"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
